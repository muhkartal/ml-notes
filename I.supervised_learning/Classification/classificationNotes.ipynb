{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decison Trees\n",
    "\n",
    "**Decision Trees (DTs)** are non-parametric supversied learning methos used for _classification_ and _regression_. The goal is o create a model that predicts the value of a target variable by learning simple decision rules inferrred from data features. **A tree** can be seen a piecewise constant approximation.\n",
    "\n",
    "For instance, in the example below decision trees learn from data to approximate a sine curve with set of **if-then-else decision rules** The deeper the tree, the more complex the decision rules and fitter the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"../Classification/data/classifcation.png\" alt=\"Scatter Plot\" width=\"600\" height=\"350\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity Of Decision Tree\n",
    "\n",
    "In general, the run time cost to construct a balanced binary tree is  _O($n_{samples}$ $n_{features}$ log($n_{samples}$))_ and query time O(log($n_{samples}$)). Although the tree construction algorithm attempt to generate balanced trees, they will not always be balanaced. Assuming that subtrees remain approximately balanced, the cost at each node consist of searching through _O($n_{samples}$ $n_{features}$ log($n_{samples}$))_ at each node, leading to total cost over the entire trees.\n",
    "\n",
    "(by summing the cost at each node) of _O($n_{samples}$ $n^{2}_{features}$ log($n_{samples}$))_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips on practical\n",
    "\n",
    "* Deci"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
