{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts of Linear Regression\n",
    "\n",
    "#### 1. Equation of a Line:\n",
    "The equation for a simple linear regression (single feature) is:\n",
    "**_y=mx+b_**\n",
    "\n",
    "where:\n",
    "* _y_ is the predicted value\n",
    "* _m_ isthe slope (coefficent of _x_),\n",
    "* _x_ is the independent variable,\n",
    "* _b_ is the intercept ( the value of _y_ when _x_ = 0)\n",
    "\n",
    "\n",
    "#### 2. Cost Function (Mean Squared Error):\n",
    "The cost function measures how well the model's predictions align with the actual data. The goal is to minimize this function:\n",
    "        \n",
    "$J_{Î¸}$ = \t$\\frac{1}{2m}$ \t$\\sum_{i = 1}^{m}$ ($J_{Î¸}$($x^{(i)}$) - $y^{(i)}$ )^2\n",
    "\n",
    "where:\n",
    "* _m_ is the number of training examples,\n",
    "* _($J_{Î¸}$($x^{(i)}$)_ is the predicted value for example _ğ‘–_\n",
    "* _($y^{(i)}$)_ is the actual value for example _ğ‘–_\n",
    "\n",
    "#### 3. Gradient Descent \n",
    "Gradient descent is an optimization algorithm used to minimize the cost function by iteratively adjusting the model parameters (ğœƒ). The update rule for each parameter is:\n",
    "\n",
    "$ğœƒ_{j}$ := $ğœƒ_{j}$ - Î± $\\frac{\\partial f}{\\partial ğœƒ_{j}}$ _J_(ğœƒ)\n",
    "\n",
    "where:\n",
    "\n",
    "* ğ›¼ is the learning rate,\n",
    "* $\\frac{\\partial f}{\\partial ğœƒ_{j}}$ _J_(ğœƒ) s the partial derivative of the cost function with respect to $ğœƒ_{j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
